{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sf api setup\n",
    "import time\n",
    "import asyncio\n",
    "import logging\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from os.path import join\n",
    "\n",
    "from sfapi_client         import Client, AsyncClient\n",
    "from sfapi_client.compute import Machine\n",
    "from sfapi_client.jobs    import JobState\n",
    "\n",
    "from sfapi_connector import KeyManager, OsSFAPI, OsWrapper, LOGGER\n",
    "import json\n",
    "import re\n",
    "import subprocess\n",
    "import webbrowser\n",
    "from io import BytesIO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The job state is: JobState.RUNNING (<enum 'JobState'>), jobid: 33089677\n",
      "+ module load conda\n",
      "+ '[' -z '' ']'\n",
      "+ case \"$-\" in\n",
      "+ __lmod_sh_dbg=x\n",
      "+ '[' -n x ']'\n",
      "+ set +x\n",
      "Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for Lmod's output\n",
      "Shell debugging restarted\n",
      "+ unset __lmod_sh_dbg\n",
      "+ return 0\n",
      "++ conda info --base\n",
      "++ local cmd=info\n",
      "++ case \"$cmd\" in\n",
      "++ __conda_exe info --base\n",
      "++ /global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/conda info --base\n",
      "+ source /global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/etc/profile.d/conda.sh\n",
      "++ export CONDA_EXE=/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/conda\n",
      "++ CONDA_EXE=/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/conda\n",
      "++ export _CE_M=\n",
      "++ _CE_M=\n",
      "++ export _CE_CONDA=\n",
      "++ _CE_CONDA=\n",
      "++ export CONDA_PYTHON_EXE=/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/python\n",
      "++ CONDA_PYTHON_EXE=/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/python\n",
      "++ '[' -z x ']'\n",
      "+ echo 'requirements.txt are at ./sfapi_test/requirements.txt'\n",
      "+ grep -q '^sfapi_dask_env '\n",
      "+ conda info --envs\n",
      "+ local cmd=info\n",
      "+ case \"$cmd\" in\n",
      "+ __conda_exe info --envs\n",
      "+ /global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/conda info --envs\n",
      "+ echo 'Activating Conda environment: sfapi_dask_env'\n",
      "+ conda activate sfapi_dask_env\n",
      "+ local cmd=activate\n",
      "+ case \"$cmd\" in\n",
      "+ __conda_activate activate sfapi_dask_env\n",
      "+ '[' -n '' ']'\n",
      "+ local ask_conda\n",
      "++ PS1=\n",
      "++ __conda_exe shell.posix activate sfapi_dask_env\n",
      "++ /global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/conda shell.posix activate sfapi_dask_env\n",
      "+ ask_conda='PS1='\\''(sfapi_dask_env) '\\''\n",
      "export PATH='\\''/global/homes/s/sanjeevc/.conda/envs/sfapi_dask_env/bin:/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/condabin:/opt/nersc/pe/bin:/global/common/software/nersc/bin:/global/common/software/nersc9/darshan/default/bin:/global/common/software/nersc9/sqs/2.0/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/compute-sanitizer:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/libnvvp:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/profilers/Nsight_Compute:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/profilers/Nsight_Systems/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/pe/craype/2.7.30/bin:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/libfabric/1.20.1/bin:/usr/local/bin:/usr/bin:/bin:/usr/lib/mit/bin:/opt/cray/pe/bin'\\''\n",
      "export CONDA_PREFIX='\\''/global/homes/s/sanjeevc/.conda/envs/sfapi_dask_env'\\''\n",
      "export CONDA_SHLVL='\\''1'\\''\n",
      "export CONDA_DEFAULT_ENV='\\''sfapi_dask_env'\\''\n",
      "export CONDA_PROMPT_MODIFIER='\\''(sfapi_dask_env) '\\''\n",
      "export CONDA_EXE='\\''/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/conda'\\''\n",
      "export _CE_M='\\'''\\''\n",
      "export _CE_CONDA='\\'''\\''\n",
      "export CONDA_PYTHON_EXE='\\''/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/python'\\'''\n",
      "+ eval 'PS1='\\''(sfapi_dask_env) '\\''\n",
      "export PATH='\\''/global/homes/s/sanjeevc/.conda/envs/sfapi_dask_env/bin:/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/condabin:/opt/nersc/pe/bin:/global/common/software/nersc/bin:/global/common/software/nersc9/darshan/default/bin:/global/common/software/nersc9/sqs/2.0/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/compute-sanitizer:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/libnvvp:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/profilers/Nsight_Compute:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/profilers/Nsight_Systems/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/pe/craype/2.7.30/bin:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/libfabric/1.20.1/bin:/usr/local/bin:/usr/bin:/bin:/usr/lib/mit/bin:/opt/cray/pe/bin'\\''\n",
      "export CONDA_PREFIX='\\''/global/homes/s/sanjeevc/.conda/envs/sfapi_dask_env'\\''\n",
      "export CONDA_SHLVL='\\''1'\\''\n",
      "export CONDA_DEFAULT_ENV='\\''sfapi_dask_env'\\''\n",
      "export CONDA_PROMPT_MODIFIER='\\''(sfapi_dask_env) '\\''\n",
      "export CONDA_EXE='\\''/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/conda'\\''\n",
      "export _CE_M='\\'''\\''\n",
      "export _CE_CONDA='\\'''\\''\n",
      "export CONDA_PYTHON_EXE='\\''/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/python'\\'''\n",
      "++ PS1='(sfapi_dask_env) '\n",
      "++ export PATH=/global/homes/s/sanjeevc/.conda/envs/sfapi_dask_env/bin:/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/condabin:/opt/nersc/pe/bin:/global/common/software/nersc/bin:/global/common/software/nersc9/darshan/default/bin:/global/common/software/nersc9/sqs/2.0/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/compute-sanitizer:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/libnvvp:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/profilers/Nsight_Compute:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/profilers/Nsight_Systems/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/pe/craype/2.7.30/bin:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/libfabric/1.20.1/bin:/usr/local/bin:/usr/bin:/bin:/usr/lib/mit/bin:/opt/cray/pe/bin\n",
      "++ PATH=/global/homes/s/sanjeevc/.conda/envs/sfapi_dask_env/bin:/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/condabin:/opt/nersc/pe/bin:/global/common/software/nersc/bin:/global/common/software/nersc9/darshan/default/bin:/global/common/software/nersc9/sqs/2.0/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/compute-sanitizer:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/cuda/12.2/libnvvp:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/profilers/Nsight_Compute:/opt/nvidia/hpc_sdk/Linux_x86_64/23.9/profilers/Nsight_Systems/bin:/opt/cray/pe/perftools/23.12.0/bin:/opt/cray/pe/papi/7.0.1.2/bin:/opt/cray/pe/craype/2.7.30/bin:/opt/cray/pe/mpich/8.1.28/ofi/gnu/12.3/bin:/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/libfabric/1.20.1/bin:/usr/local/bin:/usr/bin:/bin:/usr/lib/mit/bin:/opt/cray/pe/bin\n",
      "++ export CONDA_PREFIX=/global/homes/s/sanjeevc/.conda/envs/sfapi_dask_env\n",
      "++ CONDA_PREFIX=/global/homes/s/sanjeevc/.conda/envs/sfapi_dask_env\n",
      "++ export CONDA_SHLVL=1\n",
      "++ CONDA_SHLVL=1\n",
      "++ export CONDA_DEFAULT_ENV=sfapi_dask_env\n",
      "++ CONDA_DEFAULT_ENV=sfapi_dask_env\n",
      "++ export 'CONDA_PROMPT_MODIFIER=(sfapi_dask_env) '\n",
      "++ CONDA_PROMPT_MODIFIER='(sfapi_dask_env) '\n",
      "++ export CONDA_EXE=/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/conda\n",
      "++ CONDA_EXE=/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/conda\n",
      "++ export _CE_M=\n",
      "++ _CE_M=\n",
      "++ export _CE_CONDA=\n",
      "++ _CE_CONDA=\n",
      "++ export CONDA_PYTHON_EXE=/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/python\n",
      "++ CONDA_PYTHON_EXE=/global/common/software/nersc/pe/conda/24.1.0/Miniconda3-py311_23.11.0-2/bin/python\n",
      "+ __conda_hashr\n",
      "+ '[' -n '' ']'\n",
      "+ '[' -n '' ']'\n",
      "+ hash -r\n",
      "+ '[' -f ./sfapi_test/requirements.txt ']'\n",
      "+ echo 'Installing dependencies from ./sfapi_test/requirements.txt'\n",
      "+ pip install uv\n",
      "+ uv pip sync ./sfapi_test/requirements.txt\n",
      "Using Python 3.11.7 environment at /global/homes/s/sanjeevc/.conda/envs/sfapi_dask_env\n",
      "Resolved 177 packages in 2.22s\n",
      "Audited 177 packages in 1ms\n",
      "+ echo 'Starting scheduler...'\n",
      "+ scheduler_file=/pscratch/sd/s/sanjeevc/scheduler_file.json\n",
      "+ rm -f /pscratch/sd/s/sanjeevc/scheduler_file.json\n",
      "+ dask_pid=2190737\n",
      "+ DASK_DISTRIBUTED__COMM__TIMEOUTS__CONNECT=3600s\n",
      "+ '[' -f /pscratch/sd/s/sanjeevc/scheduler_file.json ']'\n",
      "+ DASK_DISTRIBUTED__COMM__TIMEOUTS__TCP=3600s\n",
      "+ dask-scheduler --interface hsn0 --scheduler-file /pscratch/sd/s/sanjeevc/scheduler_file.json\n",
      "+ echo 'Waiting for scheduler to start...'\n",
      "+ sleep 5\n",
      "+ '[' -f /pscratch/sd/s/sanjeevc/scheduler_file.json ']'\n",
      "+ echo 'Waiting for scheduler to start...'\n",
      "+ sleep 5\n",
      "/global/homes/s/sanjeevc/.conda/envs/sfapi_dask_env/lib/python3.11/site-packages/distributed/cli/dask_scheduler.py:142: FutureWarning: dask-scheduler is deprecated and will be removed in a future release; use `dask scheduler` instead\n",
      "  warnings.warn(\n",
      "2024-11-18 22:25:37,929 - distributed.scheduler - INFO - -----------------------------------------------\n",
      "2024-11-18 22:25:40,227 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "2024-11-18 22:25:40,260 - distributed.scheduler - INFO - State start\n",
      "2024-11-18 22:25:40,266 - distributed.scheduler - INFO - -----------------------------------------------\n",
      "2024-11-18 22:25:40,267 - distributed.scheduler - INFO -   Scheduler at:   tcp://10.249.2.147:8786\n",
      "2024-11-18 22:25:40,267 - distributed.scheduler - INFO -   dashboard at:  http://10.249.2.147:8787/status\n",
      "2024-11-18 22:25:40,270 - distributed.scheduler - INFO - Registering Worker plugin shuffle\n",
      "+ '[' -f /pscratch/sd/s/sanjeevc/scheduler_file.json ']'\n",
      "+ echo 'Scheduler started'\n",
      "+ echo 'Starting workers...'\n",
      "+ echo 'Workers started. Check /pscratch/sd/s/sanjeevc/worker_log.out for details.'\n",
      "+ DASK_DISTRIBUTED__COMM__TIMEOUTS__CONNECT=3600s\n",
      "+ DASK_DISTRIBUTED__COMM__TIMEOUTS__TCP=3600s\n",
      "+ srun dask worker --scheduler-file /pscratch/sd/s/sanjeevc/scheduler_file.json --interface hsn0 --nworkers 1\n",
      "+ echo Sleeping...\n",
      "+ sleep 3\n",
      "+ echo 'Verifying number of Dask workers...'\n",
      "+ python -c '\n",
      "from dask.distributed import Client\n",
      "client = Client(scheduler_file='\\''/pscratch/sd/s/sanjeevc/scheduler_file.json'\\'')\n",
      "print('\\''Number of workers:'\\'', len(client.scheduler_info()['\\''workers'\\'']))\n",
      "'\n",
      "2024-11-18 22:25:54,597 - distributed.scheduler - INFO - Receive client connection: Client-1ec106cd-a63f-11ef-aded-0040a6873d8a\n",
      "2024-11-18 22:25:56,879 - distributed.core - INFO - Starting established connection to tcp://10.249.2.147:36118\n",
      "2024-11-18 22:25:56,891 - distributed.scheduler - INFO - Remove client Client-1ec106cd-a63f-11ef-aded-0040a6873d8a\n",
      "2024-11-18 22:25:56,891 - distributed.core - INFO - Received 'close-stream' from tcp://10.249.2.147:36118; closing.\n",
      "2024-11-18 22:25:56,892 - distributed.scheduler - INFO - Remove client Client-1ec106cd-a63f-11ef-aded-0040a6873d8a\n",
      "2024-11-18 22:25:56,893 - distributed.scheduler - INFO - Close client connection: Client-1ec106cd-a63f-11ef-aded-0040a6873d8a\n",
      "++ hostname -f\n",
      "+ echo 'hostname: x1005c3s4b1n0h0.chn.perlmutter.nersc.gov'\n",
      "+ echo 'waiting for client connection...'\n",
      "+ wait\n",
      "\n",
      "Hostname parsed: x1005c3s4b1n0h0.chn.perlmutter.nersc.gov'\n",
      "---------------------------------\n",
      "Searching for Dask IP address...\n",
      "b'{\\n  \"type\": \"Scheduler\",\\n  \"id\": \"Scheduler-dae0ee6b-d3ac-4547-839e-a7db63167f5c\",\\n  \"address\": \"tcp://10.249.2.147:8786\",\\n  \"services\": {\\n    \"dashboard\": 8787\\n  },\\n  \"started\": 1731997537.9410095,\\n  \"workers\": {}\\n}'\n",
      "Dask IP address: 10.249.2.147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pseudo-terminal will not be allocated because stdin is not a terminal.\n",
      "***************************************************************************\n",
      "                          NOTICE TO USERS\n",
      "\n",
      "Lawrence Berkeley National Laboratory operates this computer system under \n",
      "contract to the U.S. Department of Energy.  This computer system is the \n",
      "property of the United States Government and is for authorized use only.\n",
      "Users (authorized or unauthorized) have no explicit or implicit \n",
      "expectation of privacy.\n",
      "\n",
      "Any or all uses of this system and all files on this system may be\n",
      "intercepted, monitored, recorded, copied, audited, inspected, and disclosed\n",
      "to authorized site, Department of Energy, and law enforcement personnel,\n",
      "as well as authorized officials of other agencies, both domestic and foreign.\n",
      "By using this system, the user consents to such interception, monitoring,\n",
      "recording, copying, auditing, inspection, and disclosure at the discretion\n",
      "of authorized site or Department of Energy personnel.\n",
      "\n",
      "Unauthorized or improper use of this system may result in administrative\n",
      "disciplinary action and civil and criminal penalties. By continuing to use\n",
      "this system you indicate your awareness of and consent to these terms and\n",
      "conditions of use. LOG OFF IMMEDIATELY if you do not agree to the conditions\n",
      "stated in this warning.\n",
      "\n",
      "*****************************************************************************\n",
      "\n",
      "Login connection to host x3114c0s5b0n0:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSH tunnel opened\n"
     ]
    }
   ],
   "source": [
    "# sf api send jobscript\n",
    "\n",
    "\n",
    "\n",
    "target = \"./sfapi_test\"\n",
    "# Variables\n",
    "env_name=\"sfapi_dask_env\"\n",
    "requirements_file=\"./sfapi_test/requirements.txt\"\n",
    "\n",
    "job_script = f\"\"\"#!/bin/bash\n",
    "#SBATCH -q debug\n",
    "#SBATCH -A m669\n",
    "#SBATCH -N 1\n",
    "#SBATCH -n 5              # Number of tasks (64 tasks, 32 per node)\n",
    "#SBATCH -C cpu\n",
    "#SBATCH -t 00:30:00\n",
    "#SBATCH -J sfapi-demo\n",
    "#SBATCH --exclusive\n",
    "#SBATCH --output=./sfapi_test/sfapi-demo-%j.out\n",
    "#SBATCH --error=./sfapi_test/sfapi-demo-%j.error\n",
    "\n",
    "# Print each command for debugging\n",
    "set -x\n",
    "\n",
    "\n",
    "# Load necessary modules\n",
    "module load conda\n",
    "#module load python dask\n",
    "\n",
    "# Ensure Conda is initialized\n",
    "source $(conda info --base)/etc/profile.d/conda.sh\n",
    "\n",
    "#test\n",
    "echo \"requirements.txt are at {requirements_file}\"\n",
    "\n",
    "# Check if the Conda environment exists; create or update if necessary\n",
    "if ! conda info --envs | grep -q \"^{env_name} \"; then\n",
    "    echo \"Creating Conda environment: {env_name}\"\n",
    "    conda create -y -n \"{env_name}\" python=3.11.7\n",
    "fi\n",
    "\n",
    "echo \"Activating Conda environment: {env_name}\"\n",
    "conda activate \"{env_name}\"\n",
    "\n",
    "if [ -f \"{requirements_file}\" ]; then\n",
    "    echo \"Installing dependencies from {requirements_file}\"\n",
    "    pip install uv\n",
    "    uv pip sync \"{requirements_file}\"\n",
    "else\n",
    "    echo \"No requirements.txt found at {requirements_file}. ERROR: Skipping dependency installation.\"\n",
    "fi\n",
    "\n",
    "\n",
    "# Start Dask Scheduler\n",
    "echo \"Starting scheduler...\"\n",
    "scheduler_file=$SCRATCH/scheduler_file.json\n",
    "rm -f $scheduler_file\n",
    "\n",
    "DASK_DISTRIBUTED__COMM__TIMEOUTS__CONNECT=3600s \\\n",
    "DASK_DISTRIBUTED__COMM__TIMEOUTS__TCP=3600s \\\n",
    "dask-scheduler \\\n",
    "    --interface hsn0 \\\n",
    "    --scheduler-file $scheduler_file &\n",
    "\n",
    "dask_pid=$!\n",
    "\n",
    "# Wait for the scheduler to start\n",
    "until [ -f $scheduler_file ]; do\n",
    "    echo \"Waiting for scheduler to start...\"\n",
    "    sleep 5\n",
    "done\n",
    "echo \"Scheduler started\"\n",
    "\n",
    "# Start Dask Workers\n",
    "echo \"Starting workers...\"\n",
    "DASK_DISTRIBUTED__COMM__TIMEOUTS__CONNECT=3600s \\\n",
    "DASK_DISTRIBUTED__COMM__TIMEOUTS__TCP=3600s \\\n",
    "srun dask worker \\\n",
    "    --scheduler-file $scheduler_file \\\n",
    "    --interface hsn0 \\\n",
    "    --nworkers 1 > $SCRATCH/worker_log.out 2>&1 &\n",
    "\n",
    "echo \"Workers started. Check $SCRATCH/worker_log.out for details.\"\n",
    "\n",
    "# Wait a bit to ensure workers are started\n",
    "echo \"Sleeping...\"\n",
    "sleep 3\n",
    "\n",
    "# Check number of workers\n",
    "echo \"Verifying number of Dask workers...\"\n",
    "python -c \"\n",
    "from dask.distributed import Client\n",
    "client = Client(scheduler_file='$scheduler_file')\n",
    "print('Number of workers:', len(client.scheduler_info()['workers']))\n",
    "\"\n",
    "\n",
    "# Print hostname\n",
    "echo \"hostname: $(hostname -f)\"\n",
    "\n",
    "# Wait for client connection\n",
    "echo \"waiting for client connection...\"\n",
    "wait\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "km = KeyManager()\n",
    "\n",
    "with Client(key=km.key) as client:\n",
    "    perlmutter = client.compute(Machine.perlmutter)\n",
    "\n",
    "\n",
    "\n",
    "    [path] = perlmutter.ls('/global/homes/s/sanjeevc/sfapi_test/', directory=True)\n",
    "    \n",
    "\n",
    "    # Read the file into memory as bytes\n",
    "    with open('./requirements.txt', 'rb') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # Wrap the content in BytesIO\n",
    "    file_requirements = BytesIO(file_content)\n",
    "    file_requirements.filename = 'requirements.txt'  # Add the required filename attribute\n",
    "\n",
    "    path.upload(file_requirements)\n",
    "    print(f\"Uploaded requirements.txt to {path}\")\n",
    "    \n",
    "\n",
    "    job = perlmutter.submit_job(job_script)\n",
    "    job_global = job\n",
    "    print(f\"Submitted_job: {job.jobid}\")\n",
    "    job_id = job.jobid\n",
    "\n",
    "    while True:\n",
    "        job.update()\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        print(f\"The job state is: {job.state} ({type(job.state)}), jobid: {job.jobid}\")\n",
    "        if job.state not in [JobState.PENDING, JobState.RUNNING, JobState.COMPLETING]:\n",
    "            if job.state == JobState.FAILED:\n",
    "                print(\"Job failed\")\n",
    "            elif job.state == JobState.COMPLETED:\n",
    "                print(\"Job completed\")\n",
    "                \n",
    "            elif job.state == JobState.TIMEOUT:\n",
    "                print(\"Job timeout\")\n",
    "                  \n",
    "            break\n",
    "        try: \n",
    "            output_file = perlmutter.ls(f\"/global/homes/s/sanjeevc/sfapi_test/sfapi-demo-{job_id}.error\") #todo change to relative paths\n",
    "            output_file = output_file[0]\n",
    "            with output_file.open(\"r\") as f:\n",
    "                file_content = f.read()\n",
    "                print(file_content)\n",
    "\n",
    "            # Parsing the hostname using a regex pattern #todo remove hostname search as hostname is not used\n",
    "\n",
    "            hostname_match = re.search(r\"hostname: (.+)\", file_content)\n",
    "            if hostname_match:\n",
    "                hostname = hostname_match.group(1)\n",
    "                print(f\"Hostname parsed: {hostname}\")\n",
    "                print(f\"---------------------------------\")\n",
    "                #find the dask ip for the ssh tunnel. This reads it from the scheduler file that dask makes\n",
    "                print(f\"Searching for Dask IP address...\")\n",
    "                output_file = perlmutter.ls(f\"/pscratch/sd/s/sanjeevc/scheduler_file.json\")\n",
    "                output_file = output_file[0]\n",
    "                with output_file.open(\"r\") as f:\n",
    "                    file_content = f.read()\n",
    "                    print(file_content)\n",
    "                \n",
    "                                # Parse the JSON content to extract the Dask IP\n",
    "                scheduler_info = json.loads(file_content)\n",
    "                dask_address = scheduler_info.get(\"address\", \"\")\n",
    "                daskip = re.search(r\"tcp://([\\d.]+):\", dask_address)\n",
    "\n",
    "                # Extract and print the IP address\n",
    "                if daskip:\n",
    "                    daskip = daskip.group(1)\n",
    "                    print(f\"Dask IP address: {daskip}\")\n",
    "                    \n",
    "                    \n",
    "                    # Open the SSH tunnel to perlmutter\n",
    "                    # Example usage\n",
    "                    command = [\n",
    "                        \"ssh\",\n",
    "                        \"-o\", \"ServerAliveInterval=30\", \n",
    "                        \"-l\", \"sanjeevc\",\n",
    "                        \"-i\", \"~/.ssh/nersc\",\n",
    "                        \"-L\", f\"8786:{daskip}:8786\",\n",
    "                        \"-L\", f\"8787:{daskip}:8787\",\n",
    "                        \"sanjeevc@perlmutter.nersc.gov\"\n",
    "                    ]\n",
    "                    subprocess.Popen(command)\n",
    "                    time.sleep(3)\n",
    "                    print(\"SSH tunnel opened\")\n",
    "                    webbrowser.open('http://localhost:8787/status')\n",
    "\n",
    "                   \n",
    "                   \n",
    "                   \n",
    "                   \n",
    "                    #keep tunnel open till user wants to close it and cancel the job\n",
    "                    input(\"Cancel job?\")\n",
    "                    job.cancel()\n",
    "                    print(\"Job cancelled\")\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    print(\"Dask IP address not found.\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                break\n",
    "            else:\n",
    "                print(\"Hostname not found in the file.\")\n",
    "        except Exception as e:\n",
    "            print(f\"error: {e}\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nersc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
